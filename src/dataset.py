import os
import json
import torch
import torchaudio
import torchaudio.functional as F
from torch.utils.data import Dataset
from pathlib import Path
import numpy as np

class BatDataset(Dataset):
    def __init__(self, annotations_file, data_dir, target_sample_rate=384000, n_mels=128, fixed_duration=None, augment=False):
        """
        Args:
            annotations_file (str): Path to the JSON annotations file.
            data_dir (str): Directory containing the WAV files.
            target_sample_rate (int): Sample rate to resample audio to.
            n_mels (int): Number of mel filterbanks.
            fixed_duration (float): If set, pad/crop audio to this duration (in seconds).
            augment (bool): If True, apply on-the-fly data augmentation (pitch shift, time stretch, noise).
        """
        self.data_dir = Path(data_dir)
        self.target_sample_rate = target_sample_rate
        self.n_mels = n_mels
        self.fixed_duration = fixed_duration
        self.augment = augment
        
        with open(annotations_file, 'r') as f:
            self.annotations = json.load(f)

        # Support two annotation formats:
        # 1) Complex format expected originally (dict with 'data' containing 'tags', 'recordings', 'sound_events')
        # 2) Simple list format generated by `dummy_data_gen.py` (list of {filename, species, count})
        if isinstance(self.annotations, list):
            # Build a tags_map (id -> species name) from unique species values
            species_values = sorted({a.get('species', 'unknown') for a in self.annotations})
            self.tags_map = {i: v for i, v in enumerate(species_values)}

            # Parse simple list-style annotations into samples compatible with the rest of the code
            samples = []
            for entry in self.annotations:
                filename = entry.get('filename')
                species = entry.get('species', 'unknown')
                # map species name to id
                species_id = next((k for k, v in self.tags_map.items() if v == species), -1)
                call_count = entry.get('count', 0)

                local_path = self._find_file(filename) if filename else None
                if local_path:
                    samples.append({
                        'path': str(local_path),
                        'species_id': species_id,
                        'call_count': call_count,
                        'orig_duration': None
                    })
                else:
                    print(f"Warning: File {filename} not found in {self.data_dir}")

            self.samples = samples
        else:
            self.tags_map = {tag['id']: tag['value'] for tag in self.annotations['data']['tags']}
            self.samples = self._parse_annotations()
        
        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(
            sample_rate=target_sample_rate,
            n_mels=n_mels,
            n_fft=2048,
            hop_length=512
        )
        
        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()

    def _parse_annotations(self):
        data = self.annotations['data']
        recordings = {rec['uuid']: rec for rec in data['recordings']}
        sound_events = data['sound_events']
        
        # Aggregate events per recording
        recording_events = {}
        for event in sound_events:
            rec_uuid = event['recording']
            if rec_uuid not in recording_events:
                recording_events[rec_uuid] = []
            recording_events[rec_uuid].append(event)
            
        samples = []
        for rec_uuid, rec_info in recordings.items():
            events = recording_events.get(rec_uuid, [])
            call_count = len(events)
            
            # Determine species from events (assuming single species per file for now)
            # If no events, we might need to infer from path or skip. 
            # For this task, we assume we have annotations for the 50 files.
            species_id = -1
            if events:
                # Look for species tag in events
                for event in events:
                    if event['tags']:
                        species_id = event['tags'][0] # Assuming first tag is species
                        break
            
            # If species_id is still -1, try to guess from path (fallback)
            if species_id == -1:
                if "Pip ceylonicus" in rec_info['path']:
                    species_id = 0 # Based on JSON tags: 0 is Sco. heathii/ Pip. ceylonicus
                elif "Pip tenuis" in rec_info['path']: # Guessing path structure for other species
                    species_id = 1
            
            # Find local file path
            filename = Path(rec_info['path']).name
            local_path = self._find_file(filename)
            
            if local_path:
                samples.append({
                    'path': str(local_path),
                    'species_id': species_id,
                    'call_count': call_count,
                    'orig_duration': rec_info['duration']
                })
            else:
                print(f"Warning: File {filename} not found in {self.data_dir}")
                
        return samples

    def _apply_augmentation(self, waveform):
        """
        Apply multiple random augmentations to waveform (AGGRESSIVE for small dataset):
        - Add Gaussian noise: realistic sensor noise
        - Add background noise: simulate ambient sounds
        - Volume/amplitude scaling: 0.6x to 1.4x (very aggressive)
        - Clipping distortion: frequency content variation
        - Bit depth reduction: simulate lower quality recordings
        
        Apply 3-5 augmentations per sample to maximize data diversity.
        """
        if not self.augment:
            return waveform
        
        # Apply 3-5 augmentations per sample (increased from 2-4)
        num_augmentations = np.random.randint(3, 6)
        augmentations = np.random.choice(
            ['gaussian_noise', 'pink_noise', 'volume', 'clipping', 'quantize'],
            size=min(num_augmentations, 5),
            replace=False
        )
        
        for aug in augmentations:
            if aug == 'gaussian_noise':
                # Add Gaussian noise (most common)
                noise_level = np.random.uniform(0.001, 0.015)
                noise = torch.randn_like(waveform) * noise_level
                waveform = waveform + noise
            
            elif aug == 'pink_noise':
                # Add pink noise (1/f noise - more realistic)
                noise_level = np.random.uniform(0.0005, 0.01)
                pink_noise = torch.randn(waveform.shape[0], device=waveform.device)
                # Simple pink noise simulation: cumsum + normalize
                pink_noise = torch.cumsum(pink_noise, dim=0)
                pink_noise = pink_noise / pink_noise.abs().max()
                waveform = waveform + pink_noise * noise_level
            
            elif aug == 'volume':
                # Very aggressive volume scaling: 0.6x to 1.4x
                volume_scale = np.random.uniform(0.6, 1.4)
                waveform = waveform * volume_scale
            
            elif aug == 'clipping':
                # Soft clipping distortion (adds harmonics)
                clip_threshold = np.random.uniform(0.5, 0.95)
                waveform = torch.tanh(waveform / clip_threshold) * clip_threshold
            
            elif aug == 'quantize':
                # Bit depth reduction (simulate lossy codec)
                num_bits = np.random.randint(8, 15)  # 8-14 bits
                max_val = (2 ** (num_bits - 1)) - 1
                waveform = torch.round(waveform * max_val) / max_val
        
        # Clip to prevent explosion
        waveform = torch.clamp(waveform, -1.0, 1.0)
        
        return waveform

    def _find_file(self, filename):
        # Search in data_dir recursively
        for path in self.data_dir.rglob(filename):
            return path
        return None

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        waveform, sample_rate = torchaudio.load(sample['path'])
        
        # Resample if needed
        if sample_rate != self.target_sample_rate:
            resampler = torchaudio.transforms.Resample(sample_rate, self.target_sample_rate)
            waveform = resampler(waveform)
        
        # Apply augmentation (on-the-fly, in memory)
        waveform = self._apply_augmentation(waveform)
            
        # Handle duration (pad or crop)
        if self.fixed_duration:
            target_len = int(self.fixed_duration * self.target_sample_rate)
            current_len = waveform.shape[1]
            if current_len < target_len:
                padding = target_len - current_len
                waveform = torch.nn.functional.pad(waveform, (0, padding))
            elif current_len > target_len:
                waveform = waveform[:, :target_len]
        
        # Generate MULTIPLE acoustic features for better discrimination
        
        # 1. Mel-spectrogram (original)
        mel_spec = self.mel_spectrogram(waveform)
        mel_spec_db = self.amplitude_to_db(mel_spec)
        
        # 2. MFCC (Mel-frequency cepstral coefficients) - captures timbre
        mfcc = torchaudio.transforms.MFCC(
            sample_rate=self.target_sample_rate,
            n_mfcc=40,
            melkwargs={'n_fft': 2048, 'hop_length': 512, 'n_mels': self.n_mels}
        )(waveform)
        
        # 3. Spectral features using librosa (on first channel)
        import librosa
        waveform_np = waveform[0].numpy()
        
        # Spectral centroid (brightness)
        spectral_centroid = librosa.feature.spectral_centroid(
            y=waveform_np, sr=self.target_sample_rate, n_fft=2048, hop_length=512
        )
        
        # Spectral bandwidth (spread of frequencies)
        spectral_bandwidth = librosa.feature.spectral_bandwidth(
            y=waveform_np, sr=self.target_sample_rate, n_fft=2048, hop_length=512
        )
        
        # Zero-crossing rate (noisiness)
        zero_crossing_rate = librosa.feature.zero_crossing_rate(
            y=waveform_np, frame_length=2048, hop_length=512
        )
        
        # Convert to tensors and match time dimension
        spectral_centroid = torch.from_numpy(spectral_centroid).float()
        spectral_bandwidth = torch.from_numpy(spectral_bandwidth).float()
        zero_crossing_rate = torch.from_numpy(zero_crossing_rate).float()
        
        # Pad/crop spectral features to match mel-spectrogram time dimension
        target_time = mel_spec_db.shape[-1]
        
        def pad_or_crop(tensor, target_len):
            if tensor.shape[-1] < target_len:
                padding = target_len - tensor.shape[-1]
                tensor = torch.nn.functional.pad(tensor, (0, padding))
            elif tensor.shape[-1] > target_len:
                tensor = tensor[..., :target_len]
            return tensor
        
        spectral_centroid = pad_or_crop(spectral_centroid, target_time)
        spectral_bandwidth = pad_or_crop(spectral_bandwidth, target_time)
        zero_crossing_rate = pad_or_crop(zero_crossing_rate, target_time)
        
        # Stack all features: [mel_spec, mfcc, spectral_centroid, bandwidth, zcr]
        # Normalize each feature independently
        def normalize(x):
            return (x - x.mean()) / (x.std() + 1e-9)
        
        mel_spec_norm = normalize(mel_spec_db)
        mfcc_norm = normalize(mfcc)
        
        # Combine features by stacking along frequency dimension
        combined_features = torch.cat([
            mel_spec_norm,
            mfcc_norm,
            spectral_centroid.unsqueeze(0),
            spectral_bandwidth.unsqueeze(0),
            zero_crossing_rate.unsqueeze(0)
        ], dim=1)  # Stack along frequency/feature dimension
        
        return {
            'spectrogram': combined_features,  # Now contains all features
            'species_label': torch.tensor(sample['species_id'], dtype=torch.long),
            'call_count': torch.tensor(sample['call_count'], dtype=torch.float),
            'path': sample['path']
        }
