import os
import json
import torch
import torchaudio
import torchaudio.functional as F
from torch.utils.data import Dataset
from pathlib import Path
import numpy as np

class BatDataset(Dataset):
    def __init__(self, annotations_file, data_dir, target_sample_rate=384000, n_mels=128, fixed_duration=None, augment=False):
        """
        Args:
            annotations_file (str): Path to the JSON annotations file.
            data_dir (str): Directory containing the WAV files.
            target_sample_rate (int): Sample rate to resample audio to.
            n_mels (int): Number of mel filterbanks.
            fixed_duration (float): If set, pad/crop audio to this duration (in seconds).
            augment (bool): If True, apply on-the-fly data augmentation (pitch shift, time stretch, noise).
        """
        self.data_dir = Path(data_dir)
        self.target_sample_rate = target_sample_rate
        self.n_mels = n_mels
        self.fixed_duration = fixed_duration
        self.augment = augment
        
        with open(annotations_file, 'r') as f:
            self.annotations = json.load(f)

        # Support two annotation formats:
        # 1) Complex format expected originally (dict with 'data' containing 'tags', 'recordings', 'sound_events')
        # 2) Simple list format generated by `dummy_data_gen.py` (list of {filename, species, count})
        if isinstance(self.annotations, list):
            # Build a tags_map (id -> species name) from unique species values
            species_values = sorted({a.get('species', 'unknown') for a in self.annotations})
            self.tags_map = {i: v for i, v in enumerate(species_values)}

            # Parse simple list-style annotations into samples compatible with the rest of the code
            samples = []
            for entry in self.annotations:
                filename = entry.get('filename')
                species = entry.get('species', 'unknown')
                # map species name to id
                species_id = next((k for k, v in self.tags_map.items() if v == species), -1)
                call_count = entry.get('count', 0)

                local_path = self._find_file(filename) if filename else None
                if local_path:
                    samples.append({
                        'path': str(local_path),
                        'species_id': species_id,
                        'call_count': call_count,
                        'orig_duration': None
                    })
                else:
                    print(f"Warning: File {filename} not found in {self.data_dir}")

            self.samples = samples
        else:
            self.tags_map = {tag['id']: tag['value'] for tag in self.annotations['data']['tags']}
            self.samples = self._parse_annotations()
        
        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(
            sample_rate=target_sample_rate,
            n_mels=n_mels,
            n_fft=2048,
            hop_length=512
        )
        
        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()

    def _parse_annotations(self):
        data = self.annotations['data']
        recordings = {rec['uuid']: rec for rec in data['recordings']}
        sound_events = data['sound_events']
        
        # Aggregate events per recording
        recording_events = {}
        for event in sound_events:
            rec_uuid = event['recording']
            if rec_uuid not in recording_events:
                recording_events[rec_uuid] = []
            recording_events[rec_uuid].append(event)
            
        samples = []
        for rec_uuid, rec_info in recordings.items():
            events = recording_events.get(rec_uuid, [])
            call_count = len(events)
            
            # Determine species from events (assuming single species per file for now)
            # If no events, we might need to infer from path or skip. 
            # For this task, we assume we have annotations for the 50 files.
            species_id = -1
            if events:
                # Look for species tag in events
                for event in events:
                    if event['tags']:
                        species_id = event['tags'][0] # Assuming first tag is species
                        break
            
            # If species_id is still -1, try to guess from path (fallback)
            if species_id == -1:
                if "Pip ceylonicus" in rec_info['path']:
                    species_id = 0 # Based on JSON tags: 0 is Sco. heathii/ Pip. ceylonicus
                elif "Pip tenuis" in rec_info['path']: # Guessing path structure for other species
                    species_id = 1
            
            # Find local file path
            filename = Path(rec_info['path']).name
            local_path = self._find_file(filename)
            
            if local_path:
                samples.append({
                    'path': str(local_path),
                    'species_id': species_id,
                    'call_count': call_count,
                    'orig_duration': rec_info['duration']
                })
            else:
                print(f"Warning: File {filename} not found in {self.data_dir}")
                
        return samples

    def _apply_augmentation(self, waveform):
        """
        Apply random augmentations to waveform:
        - Pitch shift: ±2 semitones
        - Time stretch: 0.9x to 1.1x speed
        - Add noise: small Gaussian noise
        - Volume change: ±20% volume
        """
        if not self.augment:
            return waveform
        
        # Randomly choose 1-3 augmentations
        num_augmentations = np.random.randint(1, 4)
        augmentations = np.random.choice(['pitch', 'stretch', 'noise', 'volume'], 
                                        size=num_augmentations, replace=False)
        
        for aug in augmentations:
            if aug == 'pitch' and np.random.rand() > 0.5:
                # Random pitch shift: -2 to +2 semitones
                n_steps = np.random.randint(-2, 3)
                waveform = F.pitch_shift(waveform, self.target_sample_rate, n_steps)
            
            elif aug == 'stretch' and np.random.rand() > 0.5:
                # Random time stretch: 0.9x to 1.1x
                rate = np.random.uniform(0.9, 1.1)
                waveform = F.speed(waveform, rate)
            
            elif aug == 'noise' and np.random.rand() > 0.5:
                # Add Gaussian noise
                noise_level = np.random.uniform(0.001, 0.01)
                noise = torch.randn_like(waveform) * noise_level
                waveform = waveform + noise
            
            elif aug == 'volume' and np.random.rand() > 0.5:
                # Random volume change: 0.8x to 1.2x
                volume_scale = np.random.uniform(0.8, 1.2)
                waveform = waveform * volume_scale
        
        return waveform

    def _find_file(self, filename):
        # Search in data_dir recursively
        for path in self.data_dir.rglob(filename):
            return path
        return None

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        waveform, sample_rate = torchaudio.load(sample['path'])
        
        # Resample if needed
        if sample_rate != self.target_sample_rate:
            resampler = torchaudio.transforms.Resample(sample_rate, self.target_sample_rate)
            waveform = resampler(waveform)
        
        # Apply augmentation (on-the-fly, in memory)
        waveform = self._apply_augmentation(waveform)
            
        # Handle duration (pad or crop)
        if self.fixed_duration:
            target_len = int(self.fixed_duration * self.target_sample_rate)
            current_len = waveform.shape[1]
            if current_len < target_len:
                padding = target_len - current_len
                waveform = torch.nn.functional.pad(waveform, (0, padding))
            elif current_len > target_len:
                waveform = waveform[:, :target_len]
        
        # Generate Spectrogram
        spec = self.mel_spectrogram(waveform)
        spec = self.amplitude_to_db(spec)
        
        return {
            'spectrogram': spec,
            'species_label': torch.tensor(sample['species_id'], dtype=torch.long),
            'call_count': torch.tensor(sample['call_count'], dtype=torch.float),
            'path': sample['path']
        }
